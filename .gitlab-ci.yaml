stages:
  - build
  - sync-e2e-benchmark
  - collector
  - cleanup


include:
  - project: 'hive/haf'
    ref: 677bd5b63ec807f279faf99a228cf743b39e6e9c
    file: '/scripts/ci-helpers/prepare_data_image_job.yml'
  # - project: 'hive/hive'
  #   ref: 5c3c086b285cab729e3e46dcec8bc0246d2d8f82
  #   file: '/scripts/ci-helpers/prepare_data_image_job.yml'


variables:
  # HIVEMIND
  RUNNER_HIVEMIND_SERVER_HTTP_PORT: 8080
  # HAF
  HAF_POSTGRES_URL: postgresql://haf_app_admin@haf-instance:5432/haf_block_log
  HAF_ADMIN_POSTGRES_URL: postgresql://haf_admin@haf-instance:5432/haf_block_log
  # FF:
  FF_NETWORK_PER_BUILD: 1
  # GIT:
  GIT_DEPTH: 1
  GIT_STRATEGY: clone
  GIT_SUBMODULE_STRATEGY: recursive
  CI_DEBUG_SERVICES: "true"
  CI_DEBUG_SERVICES: "true"
  DATA_CACHE_MAINNET: /cache/replay_data_hivemind_${CI_PIPELINE_ID}
  BLOCK_LOG_SOURCE_DIR_5M: /blockchain/block_log_5m


workflow:
  # do not create pipeline twice when merge requests is open on current branch
  rules:
    - if: '$CI_PIPELINE_SOURCE == "web"'
    - if: '$CI_MERGE_REQUEST_ID'
    - if: '$CI_OPEN_MERGE_REQUESTS'
      when: never
    - when: always


#>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>| ANCHORS |>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>

.shared_tags:
  tags: &shared_tags
    - public-runner-docker
    - hived-for-tests


.start-timer: &start-timer
  - ./scripts/ci/timer.sh start


.check-timer: &check-timer
  - ./scripts/ci/timer.sh check

#<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<| ANCHORS |<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<


#>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>| BASH SCRIPTS |>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>

.bridge_api_smoketest-script: &bridge_api_smoketest-script
  - |
    ./scripts/ci/start-api-smoketest.sh \
        localhost \
        $RUNNER_HIVEMIND_SERVER_HTTP_PORT \
        bridge_api_patterns/ \
        api_smoketest_bridge.xml \
        $RUNNER_PYTEST_WORKERS  || true


.bridge_api_smoketest_negative-script: &bridge_api_smoketest_negative-script
  - |
    ./scripts/ci/start-api-smoketest.sh \
        localhost \
        $RUNNER_HIVEMIND_SERVER_HTTP_PORT \
        bridge_api_negative/ \
        api_smoketest_bridge_negative.xml \
        $RUNNER_PYTEST_WORKERS  || true


.condenser_api_smoketest-script: &condenser_api_smoketest-script
  - |
    ./scripts/ci/start-api-smoketest.sh \
        localhost \
        $RUNNER_HIVEMIND_SERVER_HTTP_PORT \
        condenser_api_patterns/ \
        api_smoketest_condenser_api.xml \
        $RUNNER_PYTEST_WORKERS  || true


.condenser_api_smoketest_negative-script: &condenser_api_smoketest_negative-script
  - |
    ./scripts/ci/start-api-smoketest.sh \
        localhost \
        $RUNNER_HIVEMIND_SERVER_HTTP_PORT \
        condenser_api_negative/ \
        api_smoketest_condenser_api_negative.xml \
        $RUNNER_PYTEST_WORKERS  || true


.database_api_smoketest-script: &database_api_smoketest-script
  - |
    ./scripts/ci/start-api-smoketest.sh \
        localhost \
        $RUNNER_HIVEMIND_SERVER_HTTP_PORT \
        database_api_patterns/ \
        api_smoketest_database_api.xml \
        $RUNNER_PYTEST_WORKERS  || true


.database_api_smoketest_negative-script: &database_api_smoketest_negative-script
  - |
    ./scripts/ci/start-api-smoketest.sh \
        localhost \
        $RUNNER_HIVEMIND_SERVER_HTTP_PORT \
        database_api_negative/ \
        api_smoketest_database_api_negative.xml \
        $RUNNER_PYTEST_WORKERS  || true


.follow_api_smoketest-script: &follow_api_smoketest-script
  - |
    ./scripts/ci/start-api-smoketest.sh \
        localhost \
        $RUNNER_HIVEMIND_SERVER_HTTP_PORT \
        follow_api_patterns/ \
        api_smoketest_follow_api.xml \
        $RUNNER_PYTEST_WORKERS  || true


.follow_api_smoketest_negative-script: &follow_api_smoketest_negative-script
  - |
    ./scripts/ci/start-api-smoketest.sh \
        localhost \
        $RUNNER_HIVEMIND_SERVER_HTTP_PORT \
        follow_api_negative/ \
        api_smoketest_follow_api_negative.xml \
        $RUNNER_PYTEST_WORKERS  || true


.tags_api_smoketest-script: &tags_api_smoketest-script
  - |
    ./scripts/ci/start-api-smoketest.sh \
        localhost \
        $RUNNER_HIVEMIND_SERVER_HTTP_PORT \
        tags_api_negative/ \
        api_smoketest_tags_api_negative.xml \
        $RUNNER_PYTEST_WORKERS  || true


.tags_api_smoketest_negative-script: &tags_api_smoketest_negative-script
  - |
    ./scripts/ci/start-api-smoketest.sh \
        localhost \
        $RUNNER_HIVEMIND_SERVER_HTTP_PORT \
        tags_api_patterns/ \
        api_smoketest_tags_api.xml \
        $RUNNER_PYTEST_WORKERS  || true


.mock_tests-script: &mock_tests-script
  - |
    ./scripts/ci/start-api-smoketest.sh \
        localhost \
        $RUNNER_HIVEMIND_SERVER_HTTP_PORT \
        mock_tests/ \
        api_smoketest_mock_tests.xml \
        $RUNNER_PYTEST_WORKERS  || true


.hive_api_smoketest-script: &hive_api_smoketest-script
  - |
    ./scripts/ci/start-api-smoketest.sh \
        localhost \
        $RUNNER_HIVEMIND_SERVER_HTTP_PORT \
        hive_api_patterns/ \
        api_smoketest_hive_api.xml \
        $RUNNER_PYTEST_WORKERS  || true


.api-benchmark-script: &api-benchmark-script
  - |
    ./scripts/ci/start-api-benchmarks.sh \
        localhost \
        $RUNNER_HIVEMIND_SERVER_HTTP_PORT \
        $RUNNER_BENCHMARK_ITERATIONS \
        $RUNNER_PYTEST_WORKERS  || true

#<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<| BASH SCRIPTS |<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<


#>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>| JOBS |>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>

prepare_haf_image:
  stage: build
  extends: .prepare_haf_image
  variables:
    SUBMODULE_DIR: "$CI_PROJECT_DIR/haf"
    REGISTRY_USER: "$HAF_IMG_BUILDER_USER"
    REGISTRY_PASS: "$HAF_IMG_BUILDER_PASSWORD"
  tags: *shared_tags


prepare_haf_data:
  extends: .prepare_haf_data_5m
  needs:
    - job: prepare_haf_image
      artifacts: true
  stage: build
  variables:
    SUBMODULE_DIR: "$CI_PROJECT_DIR/haf"
    BLOCK_LOG_SOURCE_DIR: $BLOCK_LOG_SOURCE_DIR_5M
    CONFIG_INI_SOURCE: "$CI_PROJECT_DIR/haf/docker/config_5M.ini"
  before_script:
    - env
    - echo $DATA_CACHE
    - echo $CONFIG_INI_SOURCE
    - !reference [.prepare_haf_data_5m, before_script]
  tags:
    - hive-builder-4

cleanup_cache_manual:
  extends: .docker_image_cleanup_job
  needs: []
  stage: cleanup
  script:
    - echo "HOSTNAME is ${HOSTNAME} CI_CONCURRENT_ID ${CI_CONCURRENT_ID}"
    - du -h -d 1 /cache
    - rm /cache/replay_data_hivemind_* -rf
  resource_group: ${CI_COMMIT_SHA}
  tags:
    - hive-builder-4
  when: manual

sync_e2e_benchmark:
  image: $CI_REGISTRY_IMAGE/ci_base_image:3.8
  stage: sync-e2e-benchmark
  needs:
    - job: prepare_haf_data
      artifacts: true
  when: on_success
  services:
    - name: $HAF_IMAGE_NAME
      alias: haf-instance
      variables:
        PG_ACCESS: "
          host    haf_block_log    haf_app_admin    0.0.0.0/0    trust\n
          host    haf_block_log    haf_admin        0.0.0.0/0    trust\n
        "
        DATADIR: $DATA_CACHE_MAINNET/datadir
        SHM_DIR: $DATA_CACHE_MAINNET/shm_dir
        LOG_FILE: $CI_JOB_NAME.log
      entrypoint: ["/usr/bin/bash"]
      command: ["-c", "whoami ; df -h /dev/shm ; sudo mount -o remount,size=512m /dev/shm ; df -h /dev/shm ; exec /home/haf_admin/docker_entrypoint.sh --replay-blockchain --stop-replay-at-block=5000000"]
  variables:
    RUNNER_HIVEMIND_SYNC_MAX_BLOCK: 5000024
    HIVED_UID: $HIVED_UID
  before_script:
    - scripts/ci/fix_ci_tag.sh
    - python3 -V
    - pip3 -V
    - python3 -m venv venv/
    - . venv/bin/activate
    - pip install --upgrade pip setuptools wheel
    - pip install --no-cache-dir .[tests] 2>&1 | tee pip_install.log
    - pip list
    - env
    - |
        curl -XPOST -d '{
        "jsonrpc": "2.0",
        "method": "database_api.get_dynamic_global_properties",
        "params": {
        },
        "id": 2
        }' haf-instance:8090
    - psql "${HAF_ADMIN_POSTGRES_URL}" -c 'SELECT 1'
    - psql "${HAF_ADMIN_POSTGRES_URL}" -c 'SHOW max_parallel_workers_per_gather'
    - psql "${HAF_ADMIN_POSTGRES_URL}" -c 'SHOW data_directory'
  script:
    - echo "HAF image name $HAF_IMAGE_NAME"
    - ./scripts/ci/wait-for-postgres.sh $HAF_ADMIN_POSTGRES_URL
    - ./scripts/ci/add-mocks-to-db.sh
    - psql "${HAF_ADMIN_POSTGRES_URL}" -c 'CREATE EXTENSION IF NOT EXISTS intarray;'
    - ./scripts/ci/hive-sync.sh
    - ./scripts/ci/collect-db-stats.sh
    - ./scripts/ci/hive-server.sh start
    - *bridge_api_smoketest-script
    - *bridge_api_smoketest_negative-script
    - *condenser_api_smoketest-script
    - *condenser_api_smoketest_negative-script
    - *database_api_smoketest-script
    - *database_api_smoketest_negative-script
    - *follow_api_smoketest-script
    - *follow_api_smoketest_negative-script
    - *tags_api_smoketest-script
    - *tags_api_smoketest_negative-script
    - *mock_tests-script
    - *hive_api_smoketest-script
    - ./scripts/ci/hive-server.sh stop
    - mv ./request_process_times.log ./request_process_times_smoketests.log
    - ./scripts/ci/hive-server.sh start
    - *api-benchmark-script
  after_script:
    - cat venv/lib/python3.8/site-packages/hive/_version.py > version.log
    - echo "ARTIFACTS_JOB_ID=$CI_JOB_ID" >> variables.env
    - echo "APP_VERSION=$(git describe --tags)" >> variables.env
    - echo "SERVER_NAME=$CI_RUNNER_DESCRIPTION" >> variables.env
  artifacts:
    when: always
    expire_in: 7 days
    reports:
      junit: "*.xml"
      dotenv: variables.env
    paths:
      - pip_install.log
      - hivemind-sync.log
      - hivemind-server.log
      - pg-stats
      - tests/api_tests/hivemind/tavern/**/*.out.json
      - request_process_times.log
      - request_process_times_smoketests.log
      - version.log
  tags:
    - hive-builder-4


Trigger benchmark-results-collector:
  stage: collector
  needs: [ 'sync_e2e_benchmark' ]
  rules:
    - if: '$CI_PIPELINE_SOURCE == "merge_request_event"'
      when: never
    - if: '$CI_COMMIT_BRANCH == $CI_DEFAULT_BRANCH'
      when: on_success
    - if: '$CI_COMMIT_BRANCH == "develop"'
      when: on_success
    - when: manual
  variables:
    ARTIFACTS_URL: https://gitlab.syncad.com/api/v4/projects/$CI_PROJECT_ID/jobs/$ARTIFACTS_JOB_ID/artifacts
    PRIVATE_TOKEN: $READ_ARTIFACT_ACCESS_TOKEN
    #description:
    SOURCE: hivemind
    JOB_ID: $ARTIFACTS_JOB_ID
    DESC: "hivemind CI"
    EXEC_ENV_DESC: "branch=$CI_COMMIT_REF_SLUG"
    SERVER_NAME: "$SERVER_NAME"
    APP_VERSION: "$APP_VERSION"
    TESTSUITE_VERSION: "commit_short_sha=$CI_COMMIT_SHORT_SHA"
  trigger:
    project: hive/benchmark-results-collector
    branch: master
    strategy: depend

#<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<| JOBS |<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
